image: "${batch_docker_image}"
jobRoleArn: "${batch_job_role_arn}"
volumes:
  - name: "scratch"
    host:
      sourcePath: "${miniwdl_dir}"
  - name: "docker_sock"
    host:
      sourcePath: "/var/run/docker.sock"
mountPoints:
  - sourceVolume: "scratch"
    containerPath: "${miniwdl_dir}"
    readOnly: false
  - sourceVolume: "docker_sock"
    containerPath: "/var/run/docker.sock"
    readOnly: false
ulimits:
  - name: "nofile"
    hardLimit: 100000
    softLimit: 100000
privileged: false
readonlyRootFilesystem: false

# The AWS Batch API requires two resource quotas: vCPU and memory. Memory contention or starvation is more dangerous
# than CPU contention (an OOM condition will cause a job to fail, while lower than expected CPU will just cause it to
# run longer). The Batch scheduler uses both quotas to schedule (pack) jobs onto instances, but only enforces the memory
# quota as a hard limit. We set both quotas to a token value here. The step function overrides the memory quota at
# runtime, causing it to become the concurrency-limiting factor for job packing. (Additional logic is required in
# miniwdl to set the memory hard limits on child containers running WDL tasks.)
vcpus: 1
memory: 4
